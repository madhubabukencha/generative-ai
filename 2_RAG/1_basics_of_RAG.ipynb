{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8890ffa0-ab0b-4838-84c9-e418da3f8437",
   "metadata": {},
   "source": [
    "<p style=\"color:#153462; \n",
    "          font-weight: bold; \n",
    "          font-size: 30px; \n",
    "          font-family: Gill Sans, sans-serif;\n",
    "          text-align: center;\">\n",
    "          Retrieval - Augmeted Generation (RAG)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb5a29-bff6-4c73-a554-7e2e16058350",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word; font-size:17px;\">\n",
    "    RAG has two main components:\n",
    "    <ul style=\"text-align: justify; text-justify: inter-word; font-size:17px;\">\n",
    "        <li><b>Retriever</b>: Identifies and retrieves relavant documents</li>\n",
    "        <li><b>Generator</b>: Takes retrieved docs and the input query to generate coherent and contextually relevant response</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e3b85-b918-492d-91a1-93850d8bd0e1",
   "metadata": {},
   "source": [
    "### <span style=\"color:#C738BD; font-weight: bold;\">Definition</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96538db-5291-401c-abc3-068b2b116c3f",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; text-justify: inter-word; font-size:17px;\">\n",
    "    A framework that combines the strengths of retrieved-based systems and generation based models to produce\n",
    "    more accurate and contextual relevant response.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570b9359-b9be-4247-8d9a-044f4159f3cb",
   "metadata": {},
   "source": [
    "<img src=\"images\\rag-architecture.png\" alt=\"rag-architecture\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd25f54-e59b-4ec4-af92-d2767ea9dcb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style=\"text-align: justify; \n",
    "          text-justify: inter-word;\n",
    "          font-size:17px;\">\n",
    "    Let's break down the RAG (Retrieval Augmented Generation) architecture step-by-step. <br>\n",
    "    <ol style=\"text-align: justify; \n",
    "          text-justify: inter-word;\n",
    "          font-size:17px;\">\n",
    "        <li>\n",
    "            Indexing\n",
    "            <ul>\n",
    "                <li>\n",
    "                    <b>Documents:</b> The process begins with a collection of documents (text, code, etc.)\n",
    "                    that you want to make searchable and useable.\n",
    "                </li>\n",
    "                <li>\n",
    "                    <b>Parsed Files:</b> These documents are parsed into smaller chunks of data. \n",
    "                    This is done for efficient indexing and processing.\n",
    "                </li>\n",
    "                <li>\n",
    "                    <b>Embedding Model:</b> Each chunk of data is passed through an embedding model. This model\n",
    "                    transforms the text into numerical vectors (embeddings). These vectors represent the semantic\n",
    "                    meaning of the text.\n",
    "                </li>\n",
    "                <li>\n",
    "                    <b>Vectorization:</b> The embeddings are then stored in a vector store. This is a specialized\n",
    "                    database designed to efficiently store and search high-dimensional vectors.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>\n",
    "            Query\n",
    "            <ul>\n",
    "                <li><b>User Query:</b> A user submits a query (a question or prompt) to the system.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>\n",
    "            Query Processing\n",
    "            <ul>\n",
    "                <li> \n",
    "                    <b>Embedding Model:</b> The user query is also passed through the same embedding model as\n",
    "                the documents, generating a query vector.\n",
    "                </li>\n",
    "                <li>\n",
    "                    <b>Vectorization:</b> This query vector is then used to search the vector store.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>\n",
    "            Retrieval\n",
    "            <ul>\n",
    "                <b>Search:</b> The system uses similarity search algorithms to find the chunks of data in the vector store that are most similar to the query vector. These are the most relevant chunks of data to the user's query.\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>\n",
    "            Augmentation\n",
    "            <ul>\n",
    "                <li>                \n",
    "                    <b>Prompt:</b>The retrieved chunks of data are used to augment the user's original query.\n",
    "                    This means that the original query is combined with the relevant information from the retrieved\n",
    "                    chunks to create a more informative and contextually relevant prompt.\n",
    "                </li>\n",
    "                <li>\n",
    "                     <b>Related Docs:</b> The system can also provide the user with the actual documents from which\n",
    "                    the relevant chunks were extracted. This can help the user to explore the context further.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>\n",
    "            Generation\n",
    "            <ul>\n",
    "                <li>\n",
    "                    <b>Gen. LLM:</b> The augmented prompt is fed into a generative language model (LLM). This model \n",
    "                    is trained on a massive amount of text data and is capable of generating human-quality text.\n",
    "                </li>\n",
    "                <li>\n",
    "                    <b>Response:</b>The LLM generates a response based on the augmented prompt. This response is\n",
    "                    the final output of the RAG system.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e432e775-8153-48e1-ae67-a0a563e71f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
